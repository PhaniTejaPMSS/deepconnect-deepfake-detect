{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing required libraries"
      ],
      "metadata": {
        "id": "3k-3DdhvKKQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WsxtM7S1KEZh",
        "outputId": "230c536c-3e9a-423a-c2dd-1964097a6f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio tensorflow tensorflow-addons facenet-pytorch moviepy opencv-python torch==2.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "QRUjkx6aKb0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons\n",
        "\n",
        "from facenet_pytorch import MTCNN\n",
        "from PIL import Image\n",
        "import moviepy.editor as mp\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-uWFaLpEKIRR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount G-Drive and Extract the pre-trained model"
      ],
      "metadata": {
        "id": "XBB0NZLSPiFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "__fkiapbNe7A",
        "outputId": "0ee74fdd-c376-4251-fd04-f56fc023a509"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/DeepFake_Video_Detection/FINAL-EFFICIENTNETV2-B0.zip -d /content/saved_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "czdGW_z9QEcP",
        "outputId": "794e9ac5-98b7-426f-e0de-fbf1b1058bd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Resume_Projects/DeepFake_Video_Detection/FINAL-EFFICIENTNETV2-B0.zip\n",
            "   creating: /content/saved_model/assets/\n",
            "   creating: /content/saved_model/variables/\n",
            "  inflating: /content/saved_model/saved_model.pb  \n",
            "  inflating: /content/saved_model/keras_metadata.pb  \n",
            "  inflating: /content/saved_model/variables/variables.data-00000-of-00001  \n",
            "  inflating: /content/saved_model/variables/variables.index  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Face Detector model"
      ],
      "metadata": {
        "id": "rvc1CSbeP1lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mtcnn = MTCNN(margin=14, keep_all=True, factor=0.7, device='cpu')"
      ],
      "metadata": {
        "id": "zXhnavfl9dVE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a detection pipeline"
      ],
      "metadata": {
        "id": "kRCNfo3RP9_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionPipeline:\n",
        "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
        "\n",
        "    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n",
        "        self.detector = detector\n",
        "        self.n_frames = n_frames\n",
        "        self.batch_size = batch_size\n",
        "        self.resize = resize\n",
        "\n",
        "    def __call__(self, filename):\n",
        "        v_cap = cv2.VideoCapture(filename)\n",
        "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        if self.n_frames is None:\n",
        "            sample = np.arange(0, v_len)\n",
        "        else:\n",
        "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
        "\n",
        "        faces = []\n",
        "        frames = []\n",
        "        face2 = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "\n",
        "        for j in range(v_len):\n",
        "            success = v_cap.grab()\n",
        "            if j in sample:\n",
        "                success, frame = v_cap.retrieve()\n",
        "                if not success:\n",
        "                    continue\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                if self.resize is not None:\n",
        "                    frame = cv2.resize(frame, (int(frame.shape[1] * self.resize), int(frame.shape[0] * self.resize)))\n",
        "                frames.append(frame)\n",
        "\n",
        "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
        "                    boxes, probs = self.detector.detect(frames)\n",
        "\n",
        "                    for i in range(len(frames)):\n",
        "                        if boxes[i] is None:\n",
        "                            faces.append(face2)\n",
        "                            continue\n",
        "\n",
        "                        box = boxes[i][0].astype(int)\n",
        "                        frame = frames[i]\n",
        "                        face = frame[box[1]:box[3], box[0]:box[2]]\n",
        "\n",
        "                        if not face.any():\n",
        "                            faces.append(face2)\n",
        "                            continue\n",
        "\n",
        "                        face2 = cv2.resize(face, (224, 224))\n",
        "                        faces.append(face2)\n",
        "\n",
        "                    frames = []\n",
        "\n",
        "        v_cap.release()\n",
        "        return faces"
      ],
      "metadata": {
        "id": "BbhjvoUZ9dXy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detection_pipeline = DetectionPipeline(detector=mtcnn,n_frames=None, batch_size=60)"
      ],
      "metadata": {
        "id": "renoVIz49dai"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading saved model"
      ],
      "metadata": {
        "id": "MAmd3t-NQDdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(r\"/content/saved_model\")"
      ],
      "metadata": {
        "id": "gApmIvDQ9ddJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deepfakespredict(input_video):\n",
        "    faces = detection_pipeline(input_video)\n",
        "\n",
        "    total = 0\n",
        "    real = 0\n",
        "    fake = 0\n",
        "\n",
        "    for face in faces:\n",
        "        face2 = face / 255.0\n",
        "        pred = model.predict(np.expand_dims(face2, axis=0))[0]\n",
        "        total += 1\n",
        "\n",
        "        pred2 = pred[1]\n",
        "\n",
        "        if pred2 > 0.5:\n",
        "            fake += 1\n",
        "        else:\n",
        "            real += 1\n",
        "\n",
        "    fake_ratio = fake / total\n",
        "\n",
        "    text = \"\"\n",
        "    text2 = \"Deepfakes Confidence: \" + str(fake_ratio * 100) + \"%\"\n",
        "\n",
        "    if fake_ratio >= 0.5:\n",
        "        text = \"The video is FAKE.\"\n",
        "    else:\n",
        "        text = \"The video is REAL.\"\n",
        "\n",
        "    face_frames = []\n",
        "\n",
        "    for face in faces:\n",
        "        face_frame = Image.fromarray(face.astype('uint8'), 'RGB')\n",
        "        face_frames.append(face_frame)\n",
        "\n",
        "    face_frames[0].save('results.gif', save_all=True, append_images=face_frames[1:], duration=250, loop=100)\n",
        "    clip = mp.VideoFileClip(\"results.gif\")\n",
        "    clip.write_videofile(\"video.mp4\")\n",
        "\n",
        "    return text, text2, \"video.mp4\""
      ],
      "metadata": {
        "id": "NiDBR8hW9dfh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Gradio Interface"
      ],
      "metadata": {
        "id": "snTrOhpKQTyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"DeepConnect\"\n",
        "description = '''Verify the authenticity of your video using our advanced deepfake detection technology.'''"
      ],
      "metadata": {
        "id": "FDqxJXMg9dic"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    fn=deepfakespredict,\n",
        "    inputs=gr.Video(),\n",
        "    outputs=[gr.Textbox(label=\"Detection Result\"), gr.Textbox(label=\"Confidence Level\"), gr.Video(label=\"Detected Face Sequence\")],\n",
        "    title=title,\n",
        "    description=description\n",
        ").launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "e_JAcDnx9dnB",
        "outputId": "4ca55c7c-d5b6-4c1e-9a96-53c7604c379c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://bdc11be29f2be1f523.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bdc11be29f2be1f523.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vf07wkPdKJMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kluetZUKJPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
